{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6 - Classifiers Boosting Algorithms\n",
    "\n",
    "In this lab, we will implement the AdaBoost algorithm as an ensemble learning technique which\n",
    "aims to combine a number of weak classifiers to yield a strong classifier at the end.\n",
    "The idea of this lab is to identify whether a tumor with given characteristics is malignant or\n",
    "benign. This is a two-class classification problem.\n",
    "\n",
    "## Dataset and Features\n",
    "\n",
    "You will be working on the dataset from *Hastie et al,* for breast tumor classification with 10 features representing the tumor's:\n",
    "\n",
    "                              1. Area            6. Texture\n",
    "                              2. Perimeter       7. Symmetry\n",
    "                              3. Radius          8. Greyscale Level\n",
    "                              4. Compactness     9. Fractal Dimension\n",
    "                              5. Concavity      10. Coastline Approximation.\n",
    "There is one output variable which is diagnosis. It takes one of two values `+1` for malignant and `-1` for benign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "Why it is sometimes better to have the two class values `+1` and `-1` instead of `+1`\n",
    "and `0`?\\\n",
    "**HINT :** Think about the voting scheme at the end of the boosting algorithm. How can the class values\n",
    "affect this scheme?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    Your answer: Consider the case that we have only 2 weak classifiers. If classifier 1 voted with 1 and the \\n    second classifier voted with 0 ==> the total score is 1 which is not accurate but if the second classifier\\n    voted with -1 the final score will be 0 which more accurate as it say that 50% it's a malignant and 50% benign.\\n\\n    +1 and -1 are more representative in case of equality in votes.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Your answer: Consider the case that we have only 2 weak classifiers. If classifier 1 voted with 1 and the \n",
    "    second classifier voted with 0 ==> the total score is 1 which is not accurate but if the second classifier\n",
    "    voted with -1 the final score will be 0 which more accurate as it say that 50% it's a malignant and 50% benign.\n",
    "\n",
    "    +1 and -1 are more representative in case of equality in votes.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement\n",
    "You are required to fill the function `adaboost_classifier(Y_train, X_train, Y_test, X_test, T, clf).`\\\n",
    "This function takes as parameters:\n",
    "\n",
    "| | |\n",
    "|:---|:-|\n",
    "| **Y_train**| The target values for the training set |\n",
    "| **X_train**| The input features for the training set.|\n",
    "| **Y_test**| The target values for the test set.|\n",
    "| **Y_train**| The input features for the training set.|\n",
    "| **T**| The number of iterations of the AdaBoost Algorithm.|\n",
    "| **clf**| The classifier to be used. (In our case, we are using a decision tree stump as a base classifier). You can use any other classifier.|\n",
    "\n",
    "This function should return two values:\n",
    "- The accuracy of the model on the training set.\n",
    "- The accuracy of the model on the test set.\n",
    "\n",
    "\n",
    "#### Fair Note:\n",
    "In the explanation video, we assumed that (T) is the number of models you want to fit. However, this is not always the case. You may have a model base (like here we have decision trees) and you are allowed to use as many of it as you can. So (T) here becomes the number of iterations where your goal is to enhance the performance with as few iterations as possible. \n",
    "\n",
    "Do not get confused:\n",
    "- If your case is you have T models only, we set T = number of models to fit.\n",
    "- If you are allowed to use as many models as you can (as many decision trees as you need), then T is the number of iterations to choose. In such case, T becomes a parameter controlled by the programmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports ##\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** we prepared some utility functions to help you throughout the lab. please go and check the file *utils.py* and make sure you understand each function and know how to use it.\n",
    "\n",
    "### TODO: AdaBoost Implementation\n",
    "\n",
    "AdaBoost is an iterative algorithm that gives weights for the best classifier every iteration, updates weights of the data points, then repeats until convergence.\n",
    "\n",
    "The steps of the algorithm are:\n",
    "\n",
    "1. Initialize weights of the training examples:\n",
    "\n",
    "$$w_{m} = \\frac {1}{M}, m = 1,2,...M$$\n",
    "\n",
    "                                        M: number of training examples. \n",
    "\n",
    "2. For t=1 to $T$:\n",
    "\n",
    "    a) Select a classifier $h_{t}$ that best fits to the training data using weights $w_{m}$ of the training examples.\n",
    "\n",
    "    b) Compute error of $h_{t}$ as:\n",
    "$$err_{t} = \\frac {\\Sigma_{m=1}^{M} w_{m} \\phi (c_{m} \\neq h_{t}(x_{m}))}{\\Sigma_{m=1}^{M} w_{m}}$$\n",
    "\n",
    "    c) Compute weight of classifier:\n",
    "$$\\alpha_{t} = \\log (\\frac {1-err_{t}}{err_{t}} )$$\n",
    "\n",
    "    d) Update weights of wrongly classified examples:\n",
    "$$w_{m} = w_{m} * \\exp^{\\alpha_{t} \\phi (c_{m} \\neq h_{t}(x_{m}))}, \\space m = 1 ... M$$\n",
    "\n",
    "    e) Renormalize weights $w_{m}$\n",
    "\n",
    "\n",
    "\\\n",
    "3. Output: $C(x)= argmax_{k}\\space (\\space \\Sigma_{t=1}^{T} \\alpha_{t} * \\phi (h_{t}(x) = k)) \\space)$\n",
    "\n",
    "**Where** in step 2.B and 2.D, the $\\phi (y)$ function is called the *miss indicator* function that gives values:\n",
    "\n",
    "                                     1: if y is True\n",
    "                                     0: if y is False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost_classifier(Y_train, X_train, Y_test, X_test, T, clf):\n",
    "    # TODO: FILL THE FUNCTION with the implementation as the steps above\n",
    "    n_train, n_test = len(X_train), len(X_test)\n",
    "\n",
    "    # TODO [1]: Initialize weights\n",
    "    w = np.ones(n_train) / n_train\n",
    "\n",
    "    # TODO [2]:  Initialize the training and test data with empty array placeholders\n",
    "    # Hint: what should be their shape?\n",
    "    # predicted classes of the training examples\n",
    "    pred_train = np.zeros(n_train)\n",
    "    pred_test = np.zeros(n_test)  # predicted classes of the test examples\n",
    "\n",
    "    # TODO [3]: loop over the boosting iterations\n",
    "    for i in range(T):\n",
    "        # TODO [4]: Fit a classifier with the specific weights\n",
    "        # TODO [4.A]: fit the classifier on the training data\n",
    "        # Hint: search how sklearn.tree.DecisionTreeClassifier fits classifier on data\n",
    "        # Hint: search for parameter weights in the fit matrix\n",
    "        clf.fit(X_train, Y_train, sample_weight=w)\n",
    "\n",
    "        # TODO [4.B]: predict classes for the training data and test data\n",
    "        pred_train_i = clf.predict(X_train)\n",
    "        pred_test_i = clf.predict(X_test)\n",
    "\n",
    "        # TODO [5]: calculate the miss Indicator function\n",
    "        miss_indicator_01 = [int(x) for x in (pred_train_i != Y_train)]\n",
    "        miss_indicator_11 = [x if x == 1 else -1 for x in miss_indicator_01]\n",
    "\n",
    "        # TODO [6]: calculate the error for the current classifier (err_t)\n",
    "        err_t = np.dot(w, miss_indicator_01) / sum(w)\n",
    "\n",
    "        # TODO [7]: calculate current classifier weight (Alpha_t)\n",
    "        alpha_t = 0.5 * np.log((1 - err_t) / float(err_t))\n",
    "\n",
    "        # TODO [8]: update the weights\n",
    "        w = np.multiply(\n",
    "            w, np.exp([float(x) * alpha_t for x in miss_indicator_11]))\n",
    "\n",
    "        # TODO [9] Add to the overall predictions\n",
    "        pred_train = [sum(x) for x in zip(\n",
    "            pred_train, [x * alpha_t for x in pred_train_i])]\n",
    "        pred_test = [sum(x) for x in zip(\n",
    "            pred_test, [x * alpha_t for x in pred_test_i])]\n",
    "\n",
    "    pred_train, pred_test = np.sign(pred_train), np.sign(pred_test)\n",
    "    # TODO [10]: Return error rate in train and test set ==> Wrong, should be accuracy\n",
    "    # Hint: use function get_accuracy from utils.py\n",
    "    train_accuracy = utils.get_accuracy(pred_train, Y_train)\n",
    "    test_accuracy = utils.get_accuracy(pred_test, Y_test)\n",
    "    train_error = utils.get_error_rate(pred_train, Y_train)\n",
    "    test_error = utils.get_error_rate(pred_test, Y_test)\n",
    "    return train_accuracy, test_accuracy, train_error, test_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Boosted Classifier\n",
    "\n",
    "Now we will use the function you implemented to build a classifer.\\\n",
    "You will not change code here, only read the code below and run it to see how **AdaBoost** enhanced the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data ...\n",
      "Number of Iterations :  10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "sample_weight.shape == (10,), expected (9600,)!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\CUFE\\CMP Year 3\\Second Term\\Neural Networks\\Labs\\Neural-Netowks-Labs\\Lab 5 - AdaBoost Classifier\\Lab 5 - Boosting Algorithms.ipynb Cell 9\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CUFE/CMP%20Year%203/Second%20Term/Neural%20Networks/Labs/Neural-Netowks-Labs/Lab%205%20-%20AdaBoost%20Classifier/Lab%205%20-%20Boosting%20Algorithms.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m x_range:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CUFE/CMP%20Year%203/Second%20Term/Neural%20Networks/Labs/Neural-Netowks-Labs/Lab%205%20-%20AdaBoost%20Classifier/Lab%205%20-%20Boosting%20Algorithms.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNumber of Iterations : \u001b[39m\u001b[39m'\u001b[39m , i)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/CUFE/CMP%20Year%203/Second%20Term/Neural%20Networks/Labs/Neural-Netowks-Labs/Lab%205%20-%20AdaBoost%20Classifier/Lab%205%20-%20Boosting%20Algorithms.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     acc_i \u001b[39m=\u001b[39m adaboost_classifier(Y_train, X_train, Y_test, X_test, i, clf_tree)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CUFE/CMP%20Year%203/Second%20Term/Neural%20Networks/Labs/Neural-Netowks-Labs/Lab%205%20-%20AdaBoost%20Classifier/Lab%205%20-%20Boosting%20Algorithms.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     acc_train\u001b[39m.\u001b[39mappend(acc_i[\u001b[39m0\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CUFE/CMP%20Year%203/Second%20Term/Neural%20Networks/Labs/Neural-Netowks-Labs/Lab%205%20-%20AdaBoost%20Classifier/Lab%205%20-%20Boosting%20Algorithms.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     acc_test\u001b[39m.\u001b[39mappend(acc_i[\u001b[39m1\u001b[39m])\n",
      "\u001b[1;32md:\\CUFE\\CMP Year 3\\Second Term\\Neural Networks\\Labs\\Neural-Netowks-Labs\\Lab 5 - AdaBoost Classifier\\Lab 5 - Boosting Algorithms.ipynb Cell 9\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CUFE/CMP%20Year%203/Second%20Term/Neural%20Networks/Labs/Neural-Netowks-Labs/Lab%205%20-%20AdaBoost%20Classifier/Lab%205%20-%20Boosting%20Algorithms.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# DONE [3]: loop over the boosting iterations\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CUFE/CMP%20Year%203/Second%20Term/Neural%20Networks/Labs/Neural-Netowks-Labs/Lab%205%20-%20AdaBoost%20Classifier/Lab%205%20-%20Boosting%20Algorithms.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(T):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CUFE/CMP%20Year%203/Second%20Term/Neural%20Networks/Labs/Neural-Netowks-Labs/Lab%205%20-%20AdaBoost%20Classifier/Lab%205%20-%20Boosting%20Algorithms.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CUFE/CMP%20Year%203/Second%20Term/Neural%20Networks/Labs/Neural-Netowks-Labs/Lab%205%20-%20AdaBoost%20Classifier/Lab%205%20-%20Boosting%20Algorithms.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# DONE [4]: Fit a classifier with the specific weights\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CUFE/CMP%20Year%203/Second%20Term/Neural%20Networks/Labs/Neural-Netowks-Labs/Lab%205%20-%20AdaBoost%20Classifier/Lab%205%20-%20Boosting%20Algorithms.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# DONE [4.A]: fit the classifier on the training data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CUFE/CMP%20Year%203/Second%20Term/Neural%20Networks/Labs/Neural-Netowks-Labs/Lab%205%20-%20AdaBoost%20Classifier/Lab%205%20-%20Boosting%20Algorithms.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# Hint: search how sklearn.tree.DecisionTreeClassifier fits classifier on data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CUFE/CMP%20Year%203/Second%20Term/Neural%20Networks/Labs/Neural-Netowks-Labs/Lab%205%20-%20AdaBoost%20Classifier/Lab%205%20-%20Boosting%20Algorithms.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# Hint: search for parameter weights in the fit matrix\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/CUFE/CMP%20Year%203/Second%20Term/Neural%20Networks/Labs/Neural-Netowks-Labs/Lab%205%20-%20AdaBoost%20Classifier/Lab%205%20-%20Boosting%20Algorithms.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     clf \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39;49mfit(X_train, Y_train, w)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CUFE/CMP%20Year%203/Second%20Term/Neural%20Networks/Labs/Neural-Netowks-Labs/Lab%205%20-%20AdaBoost%20Classifier/Lab%205%20-%20Boosting%20Algorithms.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m# DONE [4.B]: predict classes for the training data and test data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CUFE/CMP%20Year%203/Second%20Term/Neural%20Networks/Labs/Neural-Netowks-Labs/Lab%205%20-%20AdaBoost%20Classifier/Lab%205%20-%20Boosting%20Algorithms.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     pred_train_t \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_train)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\tree\\_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    860\u001b[0m     \u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[0;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    890\u001b[0m         X,\n\u001b[0;32m    891\u001b[0m         y,\n\u001b[0;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\tree\\_classes.py:308\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    303\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNumber of labels=\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m does not match number of samples=\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mlen\u001b[39m(y), n_samples)\n\u001b[0;32m    305\u001b[0m     )\n\u001b[0;32m    307\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 308\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X, DOUBLE)\n\u001b[0;32m    310\u001b[0m \u001b[39mif\u001b[39;00m expanded_class_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:1769\u001b[0m, in \u001b[0;36m_check_sample_weight\u001b[1;34m(sample_weight, X, dtype, copy, only_non_negative)\u001b[0m\n\u001b[0;32m   1766\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSample weights must be 1D array or scalar\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1768\u001b[0m     \u001b[39mif\u001b[39;00m sample_weight\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m (n_samples,):\n\u001b[1;32m-> 1769\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1770\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39msample_weight.shape == \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m!\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1771\u001b[0m                 sample_weight\u001b[39m.\u001b[39mshape, (n_samples,)\n\u001b[0;32m   1772\u001b[0m             )\n\u001b[0;32m   1773\u001b[0m         )\n\u001b[0;32m   1775\u001b[0m \u001b[39mif\u001b[39;00m only_non_negative:\n\u001b[0;32m   1776\u001b[0m     check_non_negative(sample_weight, \u001b[39m\"\u001b[39m\u001b[39m`sample_weight`\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: sample_weight.shape == (10,), expected (9600,)!"
     ]
    }
   ],
   "source": [
    "#### DO NOT CHANGE CODE ####\n",
    "\n",
    "## First, read the dataset\n",
    "x,y = make_hastie_10_2()\n",
    "df = pd.DataFrame(x)\n",
    "df['Y'] = y\n",
    "print('Reading Data ...')\n",
    "\n",
    "# Split into training and test set\n",
    "train, test = train_test_split(df, test_size=0.2) # this function shuffles the data points, and splits the data into\n",
    "                                                  # 80% training set and 20% test set (indicated by test_size=0.2)\n",
    "\n",
    "\n",
    "X_train, Y_train = train.iloc[:, :-1], train.iloc[:, -1]\n",
    "X_test, Y_test = test.iloc[:, :-1], test.iloc[:, -1]\n",
    "# Fit a simple decision tree first\n",
    "clf_tree = DecisionTreeClassifier(max_depth=1, random_state=1)\n",
    "\n",
    "# Fit Adaboost classifier using a decision tree as base estimator\n",
    "# Test with different number of iterations\n",
    "acc_train, acc_test = [],[]\n",
    "x_range = range(10, 410, 50)\n",
    "for i in x_range:\n",
    "    print('Number of Iterations : ' , i)\n",
    "    acc_i = adaboost_classifier(Y_train, X_train, Y_test, X_test, i, clf_tree)\n",
    "    acc_train.append(acc_i[0])\n",
    "    acc_test.append(acc_i[1])\n",
    "\n",
    "# Compare error rate vs number of iterations\n",
    "utils.plot_accuracy(acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "Justify why the plot is the way it is (is it increasing or decreasing? why? when does it flattens out?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Your answer:\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "The number of iterations (T) is what we call a hyper parameter:\n",
    "   - Its value differs from model to model and from problem to problem.\n",
    "   - Its value is not learnt by time, it is set by the programmer.\n",
    "   \n",
    "Suggest ways to select the optimal T keeping in mind that:\n",
    "   - If T is too big, the training time is large (you loop for T times, each time takes a model to fit and this model might take hours to fit)\n",
    "   - If T is too small, the boosting might not reach the best values it can get.\n",
    "   \n",
    "   \n",
    "\n",
    "**HINT**: Look at the graph of number of iterations vs performance and search for elbow method. Try to understand it and explain what it does.\\\n",
    "**HINT**: There are other hyper-parameter selection techniques, search for them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Your answer:\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
